From 7ea368bce4c6983d4f2f7b5992568f4924e01054 Mon Sep 17 00:00:00 2001
From: anon <anon@example.com>
Date: Sun, 30 Mar 2025 11:59:15 -0400
Subject: [PATCH] [dev] : add walk_page_vma_opt patch

---
 arch/x86/include/asm/pgtable.h |  17 +-
 arch/x86/mm/pgtable.c          |  32 +++
 include/linux/mmu_notifier.h   |  34 +++
 include/linux/pagewalk.h       |   2 +
 mm/Makefile                    |   2 +-
 mm/mmu_notifier.c              |   1 +
 mm/pagewalk.c                  |   1 +
 mm/pagewalk_opt.c              | 432 +++++++++++++++++++++++++++++++++
 8 files changed, 519 insertions(+), 2 deletions(-)
 create mode 100644 mm/pagewalk_opt.c

diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index e02b179ec..77507ba69 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -169,6 +169,18 @@ static inline int pud_young(pud_t pud)
 	return pud_flags(pud) & _PAGE_ACCESSED;
 }
 
+#define p4d_young p4d_young
+static inline int p4d_young(p4d_t p4d)
+{
+	return p4d_flags(p4d) & _PAGE_ACCESSED;
+}
+
+#define pgd_young pgd_young
+static inline int pgd_young(pgd_t pgd)
+{
+	return pgd_flags(pgd) & _PAGE_ACCESSED;
+}
+
 static inline int pte_write(pte_t pte)
 {
 	/*
@@ -1339,7 +1351,10 @@ extern int pmdp_test_and_clear_young(struct vm_area_struct *vma,
 				     unsigned long addr, pmd_t *pmdp);
 extern int pudp_test_and_clear_young(struct vm_area_struct *vma,
 				     unsigned long addr, pud_t *pudp);
-
+extern int p4dp_test_and_clear_young(struct vm_area_struct *vma,
+				     unsigned long addr, p4d_t *p4dp);
+extern int pgdp_test_and_clear_young(struct vm_area_struct *vma,
+				     unsigned long addr, pgd_t *pgdp);
 #define __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH
 extern int pmdp_clear_flush_young(struct vm_area_struct *vma,
 				  unsigned long address, pmd_t *pmdp);
diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c
index 9deadf517..c97a2bdaf 100644
--- a/arch/x86/mm/pgtable.c
+++ b/arch/x86/mm/pgtable.c
@@ -1,4 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0
+#include "linux/export.h"
 #include <linux/mm.h>
 #include <linux/gfp.h>
 #include <linux/hugetlb.h>
@@ -562,6 +563,7 @@ int ptep_test_and_clear_young(struct vm_area_struct *vma,
 
 	return ret;
 }
+EXPORT_SYMBOL(ptep_test_and_clear_young);
 
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG)
 int pmdp_test_and_clear_young(struct vm_area_struct *vma,
@@ -575,6 +577,7 @@ int pmdp_test_and_clear_young(struct vm_area_struct *vma,
 
 	return ret;
 }
+EXPORT_SYMBOL(pmdp_test_and_clear_young);
 #endif
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
@@ -589,6 +592,35 @@ int pudp_test_and_clear_young(struct vm_area_struct *vma,
 
 	return ret;
 }
+EXPORT_SYMBOL(pudp_test_and_clear_young);
+#endif
+
+#ifdef CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG
+int p4dp_test_and_clear_young(struct vm_area_struct *vma,
+			      unsigned long addr, p4d_t *p4dp)
+{
+	int ret = 0;
+
+	if (p4d_young(*p4dp))
+		ret = test_and_clear_bit(_PAGE_BIT_ACCESSED,
+					 (unsigned long *)p4dp);
+
+	return ret;
+}
+EXPORT_SYMBOL(p4dp_test_and_clear_young);
+
+int pgdp_test_and_clear_young(struct vm_area_struct *vma,
+			      unsigned long addr, pgd_t *pgdp)
+{
+	int ret = 0;
+
+	if (pgd_young(*pgdp))
+		ret = test_and_clear_bit(_PAGE_BIT_ACCESSED,
+					 (unsigned long *)pgdp);
+
+	return ret;
+}
+EXPORT_SYMBOL(pgdp_test_and_clear_young);
 #endif
 
 int ptep_clear_flush_young(struct vm_area_struct *vma,
diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h
index 6e3c85760..028a1acf9 100644
--- a/include/linux/mmu_notifier.h
+++ b/include/linux/mmu_notifier.h
@@ -574,6 +574,40 @@ static inline void mmu_notifier_range_init_owner(
 	__young;							\
 })
 
+
+#define pudp_clear_young_notify(__vma, __address, __pudp)		\
+({									\
+	int __young;							\
+	struct vm_area_struct *___vma = __vma;				\
+	unsigned long ___address = __address;				\
+	__young = pudp_test_and_clear_young(___vma, ___address, __pudp);\
+	__young |= mmu_notifier_clear_young(___vma->vm_mm, ___address,	\
+					    ___address + PUD_SIZE);	\
+	__young;							\
+})
+
+#define p4dp_clear_young_notify(__vma, __address, __p4dp)		\
+({									\
+	int __young;							\
+	struct vm_area_struct *___vma = __vma;				\
+	unsigned long ___address = __address;				\
+	__young = p4dp_test_and_clear_young(___vma, ___address, __p4dp);\
+	__young |= mmu_notifier_clear_young(___vma->vm_mm, ___address,	\
+					    ___address + P4D_SIZE);	\
+	__young;							\
+})
+
+#define pgdp_clear_young_notify(__vma, __address, __pgdp)		\
+({									\
+	int __young;							\
+	struct vm_area_struct *___vma = __vma;				\
+	unsigned long ___address = __address;				\
+	__young = pgdp_test_and_clear_young(___vma, ___address, __pgdp);\
+	__young |= mmu_notifier_clear_young(___vma->vm_mm, ___address,	\
+					    ___address + PGDIR_SIZE);	\
+	__young;							\
+})
+
 /*
  * set_pte_at_notify() sets the pte _after_ running the notifier.
  * This is safe to start by updating the secondary MMUs, because the primary MMU
diff --git a/include/linux/pagewalk.h b/include/linux/pagewalk.h
index 27cd1e59c..1c3ee3071 100644
--- a/include/linux/pagewalk.h
+++ b/include/linux/pagewalk.h
@@ -126,6 +126,8 @@ int walk_page_range_vma(struct vm_area_struct *vma, unsigned long start,
 			void *private);
 int walk_page_vma(struct vm_area_struct *vma, const struct mm_walk_ops *ops,
 		void *private);
+int walk_page_vma_opt(struct vm_area_struct *vma, const struct mm_walk_ops *ops,
+		      void *private);
 int walk_page_mapping(struct address_space *mapping, pgoff_t first_index,
 		      pgoff_t nr, const struct mm_walk_ops *ops,
 		      void *private);
diff --git a/mm/Makefile b/mm/Makefile
index ec65984e2..3b2402721 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -38,7 +38,7 @@ CFLAGS_init-mm.o += $(call cc-disable-warning, initializer-overrides)
 mmu-y			:= nommu.o
 mmu-$(CONFIG_MMU)	:= highmem.o memory.o mincore.o \
 			   mlock.o mmap.o mmu_gather.o mprotect.o mremap.o \
-			   msync.o page_vma_mapped.o pagewalk.o \
+			   msync.o page_vma_mapped.o pagewalk.o pagewalk_opt.o \
 			   pgtable-generic.o rmap.o vmalloc.o
 
 
diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c
index ec3b068cb..3eb141013 100644
--- a/mm/mmu_notifier.c
+++ b/mm/mmu_notifier.c
@@ -401,6 +401,7 @@ int __mmu_notifier_clear_young(struct mm_struct *mm,
 
 	return young;
 }
+EXPORT_SYMBOL(__mmu_notifier_clear_young);
 
 int __mmu_notifier_test_young(struct mm_struct *mm,
 			      unsigned long address)
diff --git a/mm/pagewalk.c b/mm/pagewalk.c
index b7d7e4fcf..ae9ca39ca 100644
--- a/mm/pagewalk.c
+++ b/mm/pagewalk.c
@@ -599,6 +599,7 @@ int walk_page_vma(struct vm_area_struct *vma, const struct mm_walk_ops *ops,
 	process_vma_walk_lock(vma, ops->walk_lock);
 	return __walk_page_range(vma->vm_start, vma->vm_end, &walk);
 }
+EXPORT_SYMBOL(walk_page_vma);
 
 /**
  * walk_page_mapping - walk all memory areas mapped into a struct address_space.
diff --git a/mm/pagewalk_opt.c b/mm/pagewalk_opt.c
new file mode 100644
index 000000000..61c4f0d4d
--- /dev/null
+++ b/mm/pagewalk_opt.c
@@ -0,0 +1,432 @@
+// SPDX-License-Identifier: GPL-2.0
+#include "asm/pgtable.h"
+#include <linux/export.h>
+#include <linux/pagewalk.h>
+#include <linux/highmem.h>
+#include <linux/sched.h>
+#include <linux/hugetlb.h>
+
+/*
+ * We want to know the real level where a entry is located ignoring any
+ * folding of levels which may be happening. For example if p4d is folded then
+ * a missing entry found at level 1 (p4d) is actually at level 0 (pgd).
+ */
+static int real_depth(int depth)
+{
+	if (depth == 3 && PTRS_PER_PMD == 1)
+		depth = 2;
+	if (depth == 2 && PTRS_PER_PUD == 1)
+		depth = 1;
+	if (depth == 1 && PTRS_PER_P4D == 1)
+		depth = 0;
+	return depth;
+}
+
+static int walk_pte_range_inner(pte_t *pte, unsigned long addr,
+				unsigned long end, struct mm_walk *walk)
+{
+	const struct mm_walk_ops *ops = walk->ops;
+	int err = 0;
+
+	for (;;) {
+		err = ops->pte_entry(pte, addr, addr + PAGE_SIZE, walk);
+		if (err)
+			break;
+		if (addr >= end - PAGE_SIZE)
+			break;
+		addr += PAGE_SIZE;
+		pte++;
+	}
+	return err;
+}
+
+static int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
+			  struct mm_walk *walk)
+{
+	pte_t *pte;
+	int err = 0;
+	spinlock_t *ptl;
+
+	if (walk->no_vma) {
+		/*
+		 * pte_offset_map() might apply user-specific validation.
+		 * Indeed, on x86_64 the pmd entries set up by init_espfix_ap()
+		 * fit its pmd_bad() check (_PAGE_NX set and _PAGE_RW clear),
+		 * and CONFIG_EFI_PGT_DUMP efi_mm goes so far as to walk them.
+		 */
+		if (walk->mm == &init_mm || addr >= TASK_SIZE)
+			pte = pte_offset_kernel(pmd, addr);
+		else
+			pte = pte_offset_map(pmd, addr);
+		if (pte) {
+			err = walk_pte_range_inner(pte, addr, end, walk);
+			if (walk->mm != &init_mm && addr < TASK_SIZE)
+				pte_unmap(pte);
+		}
+	} else {
+		pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);
+		if (pte) {
+			err = walk_pte_range_inner(pte, addr, end, walk);
+			pte_unmap_unlock(pte, ptl);
+		}
+	}
+	if (!pte)
+		walk->action = ACTION_AGAIN;
+	return err;
+}
+
+#ifdef CONFIG_ARCH_HAS_HUGEPD
+static int walk_hugepd_range(hugepd_t *phpd, unsigned long addr,
+			     unsigned long end, struct mm_walk *walk,
+			     int pdshift)
+{
+	int err = 0;
+	const struct mm_walk_ops *ops = walk->ops;
+	int shift = hugepd_shift(*phpd);
+	int page_size = 1 << shift;
+
+	if (!ops->pte_entry)
+		return 0;
+
+	if (addr & (page_size - 1))
+		return 0;
+
+	for (;;) {
+		pte_t *pte;
+
+		spin_lock(&walk->mm->page_table_lock);
+		pte = hugepte_offset(*phpd, addr, pdshift);
+		err = ops->pte_entry(pte, addr, addr + page_size, walk);
+		spin_unlock(&walk->mm->page_table_lock);
+
+		if (err)
+			break;
+		if (addr >= end - page_size)
+			break;
+		addr += page_size;
+	}
+	return err;
+}
+#else
+static int walk_hugepd_range(hugepd_t *phpd, unsigned long addr,
+			     unsigned long end, struct mm_walk *walk,
+			     int pdshift)
+{
+	return 0;
+}
+#endif
+
+static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,
+			  struct mm_walk *walk)
+{
+	pmd_t *pmd;
+	unsigned long next;
+	const struct mm_walk_ops *ops = walk->ops;
+	int err = 0;
+	int depth = real_depth(3);
+
+	pmd = pmd_offset(pud, addr);
+	do {
+again:
+		next = pmd_addr_end(addr, end);
+		if (pmd_none(*pmd)) {
+			if (ops->pte_hole)
+				err = ops->pte_hole(addr, next, depth, walk);
+			if (err)
+				break;
+			continue;
+		}
+        if (!pmd_young(*pmd) || !pmd_present(*pmd))
+            continue;
+
+		walk->action = ACTION_SUBTREE;
+
+		/*
+		 * This implies that each ->pmd_entry() handler
+		 * needs to know about pmd_trans_huge() pmds
+		 */
+		if (ops->pmd_entry)
+			err = ops->pmd_entry(pmd, addr, next, walk);
+		if (err)
+			break;
+
+		if (walk->action == ACTION_AGAIN)
+			goto again;
+
+		/*
+		 * Check this here so we only break down trans_huge
+		 * pages when we _need_ to
+		 */
+		if ((!walk->vma && (pmd_leaf(*pmd) || !pmd_present(*pmd))) ||
+		    walk->action == ACTION_CONTINUE || !(ops->pte_entry))
+			continue;
+
+		if (walk->vma)
+			split_huge_pmd(walk->vma, pmd, addr);
+
+		if (is_hugepd(__hugepd(pmd_val(*pmd))))
+			err = walk_hugepd_range((hugepd_t *)pmd, addr, next,
+						walk, PMD_SHIFT);
+		else
+			err = walk_pte_range(pmd, addr, next, walk);
+		if (err)
+			break;
+
+		if (walk->action == ACTION_AGAIN)
+			goto again;
+
+	} while (pmd++, addr = next, addr != end);
+
+	return err;
+}
+
+static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,
+			  struct mm_walk *walk)
+{
+	pud_t *pud;
+	unsigned long next;
+	const struct mm_walk_ops *ops = walk->ops;
+	int err = 0;
+	int depth = real_depth(2);
+
+	pud = pud_offset(p4d, addr);
+	do {
+again:
+		next = pud_addr_end(addr, end);
+		if (pud_none(*pud)) {
+			if (ops->pte_hole)
+				err = ops->pte_hole(addr, next, depth, walk);
+			if (err)
+				break;
+			continue;
+		}
+        if (!pud_young(*pud) || !pud_present(*pud))
+            continue;
+
+		walk->action = ACTION_SUBTREE;
+
+		if (ops->pud_entry)
+			err = ops->pud_entry(pud, addr, next, walk);
+		if (err)
+			break;
+
+		if (walk->action == ACTION_AGAIN)
+			goto again;
+
+		if ((!walk->vma && (pud_leaf(*pud) || !pud_present(*pud))) ||
+		    walk->action == ACTION_CONTINUE ||
+		    !(ops->pmd_entry || ops->pte_entry))
+			continue;
+
+		if (walk->vma)
+			split_huge_pud(walk->vma, pud, addr);
+		if (pud_none(*pud))
+			goto again;
+
+		if (is_hugepd(__hugepd(pud_val(*pud))))
+			err = walk_hugepd_range((hugepd_t *)pud, addr, next,
+						walk, PUD_SHIFT);
+		else
+			err = walk_pmd_range(pud, addr, next, walk);
+		if (err)
+			break;
+	} while (pud++, addr = next, addr != end);
+
+	return err;
+}
+
+static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,
+			  struct mm_walk *walk)
+{
+	p4d_t *p4d;
+	unsigned long next;
+	const struct mm_walk_ops *ops = walk->ops;
+	int err = 0;
+	int depth = real_depth(1);
+
+	p4d = p4d_offset(pgd, addr);
+	do {
+		next = p4d_addr_end(addr, end);
+		if (p4d_none_or_clear_bad(p4d)) {
+			if (ops->pte_hole)
+				err = ops->pte_hole(addr, next, depth, walk);
+			if (err)
+				break;
+			continue;
+		}
+        if (!p4d_young(*p4d) || !p4d_present(*p4d))
+            continue;
+		if (ops->p4d_entry) {
+			err = ops->p4d_entry(p4d, addr, next, walk);
+			if (err)
+				break;
+		}
+		if (is_hugepd(__hugepd(p4d_val(*p4d))))
+			err = walk_hugepd_range((hugepd_t *)p4d, addr, next,
+						walk, P4D_SHIFT);
+		else if (ops->pud_entry || ops->pmd_entry || ops->pte_entry)
+			err = walk_pud_range(p4d, addr, next, walk);
+		if (err)
+			break;
+	} while (p4d++, addr = next, addr != end);
+
+	return err;
+}
+
+static int walk_pgd_range(unsigned long addr, unsigned long end,
+			  struct mm_walk *walk)
+{
+	pgd_t *pgd;
+	unsigned long next;
+	const struct mm_walk_ops *ops = walk->ops;
+	int err = 0;
+
+	if (walk->pgd)
+		pgd = walk->pgd + pgd_index(addr);
+	else
+		pgd = pgd_offset(walk->mm, addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		if (pgd_none_or_clear_bad(pgd)) {
+			if (ops->pte_hole)
+				err = ops->pte_hole(addr, next, 0, walk);
+			if (err)
+				break;
+			continue;
+		}
+		if (!pgd_present(*pgd) || !pgd_young(*pgd))
+			continue;
+		if (ops->pgd_entry) {
+			err = ops->pgd_entry(pgd, addr, next, walk);
+			if (err)
+				break;
+		}
+		if (is_hugepd(__hugepd(pgd_val(*pgd))))
+			err = walk_hugepd_range((hugepd_t *)pgd, addr, next,
+						walk, PGDIR_SHIFT);
+		else if (ops->p4d_entry || ops->pud_entry || ops->pmd_entry ||
+			 ops->pte_entry)
+			err = walk_p4d_range(pgd, addr, next, walk);
+		if (err)
+			break;
+	} while (pgd++, addr = next, addr != end);
+
+	return err;
+}
+
+#ifdef CONFIG_HUGETLB_PAGE
+static unsigned long hugetlb_entry_end(struct hstate *h, unsigned long addr,
+				       unsigned long end)
+{
+	unsigned long boundary = (addr & huge_page_mask(h)) + huge_page_size(h);
+	return boundary < end ? boundary : end;
+}
+
+static int walk_hugetlb_range(unsigned long addr, unsigned long end,
+			      struct mm_walk *walk)
+{
+	struct vm_area_struct *vma = walk->vma;
+	struct hstate *h = hstate_vma(vma);
+	unsigned long next;
+	unsigned long hmask = huge_page_mask(h);
+	unsigned long sz = huge_page_size(h);
+	pte_t *pte;
+	const struct mm_walk_ops *ops = walk->ops;
+	int err = 0;
+
+	hugetlb_vma_lock_read(vma);
+	do {
+		next = hugetlb_entry_end(h, addr, end);
+		pte = hugetlb_walk(vma, addr & hmask, sz);
+		if (pte)
+			err = ops->hugetlb_entry(pte, hmask, addr, next, walk);
+		else if (ops->pte_hole)
+			err = ops->pte_hole(addr, next, -1, walk);
+		if (err)
+			break;
+	} while (addr = next, addr != end);
+	hugetlb_vma_unlock_read(vma);
+
+	return err;
+}
+
+#else /* CONFIG_HUGETLB_PAGE */
+static int walk_hugetlb_range(unsigned long addr, unsigned long end,
+			      struct mm_walk *walk)
+{
+	return 0;
+}
+
+#endif /* CONFIG_HUGETLB_PAGE */
+
+static int __walk_page_range(unsigned long start, unsigned long end,
+			     struct mm_walk *walk)
+{
+	int err = 0;
+	struct vm_area_struct *vma = walk->vma;
+	const struct mm_walk_ops *ops = walk->ops;
+
+	if (ops->pre_vma) {
+		err = ops->pre_vma(start, end, walk);
+		if (err)
+			return err;
+	}
+
+	if (is_vm_hugetlb_page(vma)) {
+		if (ops->hugetlb_entry)
+			err = walk_hugetlb_range(start, end, walk);
+	} else
+		err = walk_pgd_range(start, end, walk);
+
+	if (ops->post_vma)
+		ops->post_vma(walk);
+
+	return err;
+}
+
+static inline void process_mm_walk_lock(struct mm_struct *mm,
+					enum page_walk_lock walk_lock)
+{
+	if (walk_lock == PGWALK_RDLOCK)
+		mmap_assert_locked(mm);
+	else
+		mmap_assert_write_locked(mm);
+}
+
+static inline void process_vma_walk_lock(struct vm_area_struct *vma,
+					 enum page_walk_lock walk_lock)
+{
+#ifdef CONFIG_PER_VMA_LOCK
+	switch (walk_lock) {
+	case PGWALK_WRLOCK:
+		vma_start_write(vma);
+		break;
+	case PGWALK_WRLOCK_VERIFY:
+		vma_assert_write_locked(vma);
+		break;
+	case PGWALK_RDLOCK:
+		/* PGWALK_RDLOCK is handled by process_mm_walk_lock */
+		break;
+	}
+#endif
+}
+
+int walk_page_vma_opt(struct vm_area_struct *vma, const struct mm_walk_ops *ops,
+		      void *private)
+{
+	struct mm_walk walk = {
+		.ops = ops,
+		.mm = vma->vm_mm,
+		.vma = vma,
+		.private = private,
+	};
+
+	if (!walk.mm)
+		return -EINVAL;
+
+	process_mm_walk_lock(walk.mm, ops->walk_lock);
+	process_vma_walk_lock(vma, ops->walk_lock);
+	return __walk_page_range(vma->vm_start, vma->vm_end, &walk);
+}
+
+EXPORT_SYMBOL(walk_page_vma_opt);
-- 
2.43.0


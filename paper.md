
# vtism

## introduction

在不断发展的计算领域，数据密集型应用，如人工智能、大数据分析和基于云的服务的指数级增长对内存系统提出了前所未有的要求。传统的单层内存架构主要依赖于动态随机存取内存（DRAM），很难跟上这些不断升级的要求。DRAM 的局限性，包括相对较高的每比特成本、有限的容量可扩展性和能源效率低下，已成为实现高性能和低成本计算解决方案的重大瓶颈。

为应对这些挑战，多层内存系统已成为一种前景广阔的替代方案。这些系统将不同类型的内存整合到一个分层结构中，每种内存在访问延迟、容量和成本方面都有不同的特点。在低层，快速但昂贵的 DRAM 可为常用数据提供低延迟访问，确保关键操作的快速响应时间。在中层或高层，非易失性内存（NVM）或其他新兴内存技术以较低的每比特成本提供较高的容量，尽管访问延迟较高。这种分层安排可以更有效地利用内存资源，平衡性能与成本，满足不同应用和数据访问模式的特定要求。

在多层内存系统的各种组件中，Compute Express Link（CXL）内存备受关注。CXL 是一种开放式标准互连协议，可实现对远程内存资源的直接和高速缓存一致性访问。该技术在现代计算中具有几个关键优势。首先，它能在 CPU 和内存设备之间提供高带宽通信，这对于需要快速数据传输的应用（如内存数据库和大规模数据处理）至关重要。其次，CXL 内存支持内存池和共享等功能，允许在多个计算节点之间更灵活地分配资源。这在虚拟化环境中尤其有益，因为在这种环境中，多个虚拟机（VM）需要高效地访问和共享内存资源。此外，CXL 的高速缓存一致性机制可确保不同内存层之间的数据一致性，从而降低内存管理的复杂性，提高系统的整体性能[3, 4]。

分层内存管理包括一系列旨在优化不同内存层利用率的技术和策略。页面放置是一个关键环节，它涉及根据每个页面的访问频率和数据特征，为其确定最合适的内存层。例如，访问频率高的热页面通常放置在 DRAM 中，以尽量减少访问延迟，而冷页面则可以迁移到延迟较高、成本较低的内存层。页面迁移是另一个重要的组成部分，可以随着应用访问模式的变化，在不同内存层之间移动页面。这有助于平衡整个内存层的负载，提高系统的整体性能。此外，内存管理还包括缓存管理、内存分配和去分配等考虑因素，以确保有效利用资源。

然而，在虚拟化环境中应用分层内存系统时，仍有几个难题尚未解决。其中一个主要问题是跨多个虚拟机和不同内存层管理高速缓存一致性的复杂性。在虚拟化环境中，多个虚拟机共享物理内存资源，保持缓存一致性变得更加复杂。要确保所有虚拟机对不同内存层中的数据拥有一致的视图，同时又不产生过多的开销，这是一个巨大的挑战。现有的缓存一致性协议往往难以在这种多租户环境中有效扩展，从而导致潜在的数据不一致和性能下降。

另一个挑战在于如何有效处理内存访问延迟差异。不同内存层的不同访问延迟会严重影响虚拟机的性能。由于在虚拟机上运行的应用程序具有不同的内存访问模式，因此很难预测和优化这些延迟变化。例如，一些应用程序从远程内存层访问数据时可能会出现大量缓存未命中，从而导致处理延迟增加。开发能够实时适应这些延迟差异并优化每个虚拟机内存访问的智能算法和技术至关重要，但这仍是一个有待解决的问题。

此外，虚拟化环境的动态特性（虚拟机的数量及其资源需求可能会迅速变化）也给高效的内存访问带来了挑战。

## background and motivation

现有的分层内存管理系统可以较好的平衡系统中异构内存之间的页面追踪, 分类和迁移, 但是在虚拟化场景下有一些特殊的挑战 因此需要专为虚拟化设计的分层内存管理系统

1. 分层内存管理的最终目的是充分利用异构内存的容量，性能的差异，合理分配和迁移页面使系统性能提高，直观表现体现在运行在操作系统中的进程性能得到提升。但是在虚拟化场景下，大部分云厂商选择利用一台物理服务器的CPU和内存创建许多个虚拟机供外部服务，在这个场景下虚拟机管理程序VMM作为中间层管理每一个虚拟机，虚拟化场景下的根本目标并不是利用分层内存管理使得VMM性能提高，而是使得VMM管理的每一个guest os的性能提高。guest os 并不直接受 host os 管理，它更像是一个 VMM 进程中的 OS 进程，因此，虚拟化场景下，分层内存管理系统的设计目标应该更加关注 guest os 的性能，而不是 VMM 的性能。

   因此需要精准定位 guest os 中的热点页面而不是 VMM 进程的热点页面

2. 在宿主机中,我们无法直接得知虚拟机中的真实页面类型,因为页面类型的判断是基于页面缓存(page cache)进行的,而这些信息仅由虚拟机的内核(guest kernel)掌握。宿主机通过扫描VMM进程页表的方式判断页面类型是不准确的,具体的页面类型和管理策略仍由虚拟机内部的操作系统负责

统计了 liblinear， 使用了 HIGGS 数据集训练，结果证明宿主机会将所有 VMM 进程的页面均视作匿名页面， 而在虚拟机中统计结果发现其实还有来自 HIGGS 的大量的文件页没有被正确识别，linux内核中对于文件页和匿名页的回收策略有所不同，通常对于文件页的回收要比匿名页更加激进，大部分现有的工作也都考虑到了文件页和匿名页的差异来决定回收策略

![](result/track/host_guest_file_anon/anon_pages_comparison.png)

redis

![](./result/track/gpt-vs-ept/gpt-vs-ept.png)

### State-of-the-art and limitations

## vtism design and implementation

### overview

### dynamic page tracking

Linux 的页面迁移有同步迁移和异步迁移两种模式，同步迁移先尝试使用MIGRATE_ASYNC模式批量迁移folios

如果批量迁移失败,则退回到逐个同步迁移每个folio [code](https://github.com/torvalds/linux/blob/ffc253263a1375a65fa6c9f62a893e9767fbebfa/mm/migrate.c#L1825)。

但是我们注意到现在 linux 内核使用的异步迁移不是真正的异步操作，依然是同步顺序的 copy 页面并在循环中调用 cond_resched[code](https://github.com/torvalds/linux/blob/ffc253263a1375a65fa6c9f62a893e9767fbebfa/mm/util.c#L790C1-L801C2)

多线程异步迁移，针对 页面提升（hot page migration） 和 页面降级（cold page demotion） 进行优化。思路是使用 工作队列（workqueue） 进行异步提交，并在后台完成页面迁移。

在内核中，页面迁移（Page Migration）发生的原因多种多样，不同的迁移场景对执行顺序和阻塞行为有不同的要求。其中，一些页面迁移操作需要保持严格的执行顺序，因此必须同步进行并阻塞相关进程。例如内存规整（MR_COMPACTION），内存热插拔（MR_MEMORY_HOTPLUG）系统调用（MR_SYSCALL），调用 mbind 系统调用设置 memory policy 时触发的迁移（MR_MEMPOLICY_MBIND）等

另一方面，某些迁移类型主要用于内核在系统层面优化内存访问模式，因而无需同步执行，可以采用异步机制在后台完成，以减少对前台任务的影响。例如页面提升（MR_NUMA_MISPLACED）和页面降级 （MR_DEMOTION）. 对于这些无需严格同步的迁移任务，内核可以采用异步方式进行后台迁移，以避免影响前台应用程序的执行效率。这种异步迁移策略可以提高系统的整体性能，同时保证内存资源的高效利用。

页面提升和页面降级的特性不同，因此可以采用 不同的队列调度策略

页面提升的目标是快速迁移高频访问的页到更快的存储（如 DRAM），先检查 目标 NUMA node 是否有足够的可用内存，避免迁移后导致回退。


页面降级的目标是迁移低访问频率的页面到较慢的存储。 低优先级的后台线程可以在内存压力较低时异步迁移，避免影响前台进程性能，采用 时间窗口（time window）+ 访问统计，避免短期冷热变化导致频繁迁移（MGLRU）

采用提升和降级双工作队列，不同的优先级任务交给不同的 worker。高优先级队列：处理页面提升，尽快将热点页面迁移到 DRAM，设置WQ_HIGHPRI：让热页迁移尽快完成，减少性能抖动；低优先级队列：处理页面降级，在 内存压力较低 时进行，设置WQ_UNBOUND：让调度更灵活，充分利用多核 CPU 资源。

减少锁争用：使用 RCU 避免页表锁，避免高并发时 mmap_lock 带来的性能瓶颈；冷页迁移可以使用 trylock，减少锁等待对应用程序的影响

预取优化：页面提升前，可以先检查目标地址是否存在 TLB entry，如果不存在，提前 prefetch 目标页面（prefetchw(new_page);）可以迁移后新页面的 TLB miss，提升访问速度。

活跃进程对其活跃页面的访问是影响系统性能的关键因素。如果这些活跃页面位于慢速内存节点（如 CXL-DRAM 或 NVM），我们希望尽快将其迁移到高速内存节点（如 DRAM），以降低访问延迟。然而，页面迁移的前提是目标节点具备足够的可用内存，否则迁移操作将会失败，从而影响性能优化的效果。因此，在执行页面迁移时，确保目标节点的可用内存充足是至关重要的。

为了解决这一问题，内核采用 空闲页面回收水位线（Watermark for Page Reclaim） 机制来决定何时启动页面回收。当系统空闲内存下降到设定的水位线以下时，内核会触发 页面降级（Page Demotion），即将 DRAM 中的冷页面迁移到 CXL-DRAM 或 NVM 等较慢的存储介质，以释放更多的 DRAM 资源。这一机制确保了当高优先级进程需要更多高速内存时，系统可以及时提供可用的 DRAM 页面，进而提高页面提升（Page Promotion）的成功率和效率。

由于内存回收和页面降级涉及大量的数据迁移，其执行需要一定的时间。因此，系统必须提前预留足够的空闲内存空间，以保障后续页面提升操作能够顺利进行。换句话说，提高 页面提升的成功率和执行效率 是提升系统性能的核心目标，而更主动地进行 提前页面降级 则是实现这一目标的重要保障。通过合理调整页面回收策略，使降级与提升协同工作，可以有效优化内存资源的分配，提高整体系统性能。

将内存页面从源位置迁移到目的位置需要四步：
（1）在目标节点中分配新页面
 (2) 取消映射要迁移的页面（包括使PTE失效）
 (3) 将页面从源节点复制到目标节点
（4）映射新页面（包括更新PTE）

> (1) allocate new pages in the target node; (2) unmap pages to migrate (including invalidating PTE); (3) copy pages from source to target node; (4) map new pages (including updating PTE)



## evaluation and analysis

### Experimental Setup

| **Component**  | **Specification**  |
|---------------|------------------|
| **CPU**       | 2× Intel® Xeon® Platinum 8468V CPUs @3.8 GHz, 48 cores per socket, 192 total CPUs, Hyper-Threading disabled |
| **Memory**    | **NUMA Node 0 & 1:** DDR5-4800, 32GB per channel, Hynix DIMM modules <br> **NUMA Node 2 & 3:** CXL-DRAM, 64GB, Hynix DIMM modules |
| **NUMA Nodes** | Node 0: CPU 0-47, 32GB DDR5-4800 <br> Node 1: CPU 48-95, 32GB DDR5-4800 <br> Node 2: CXL-DRAM, 64GB <br> Node 3: CXL-DRAM, 64GB |
| **Virtualization** | VT-x support |         

- autonuma 
- tpp
- nomad

Hemem PEBS 虚拟化不合适

| **Benchmark**  | **Category**                         | **RSS**  |
|---------------|--------------------------------------|---------|
| Redis (YCSB)  | Key-Value Store                     | 29 GB   |
| PR            | Graph Processing                    | 21 GB   |
| Graph500      | Graph Processing                    | 15 GB   |
| XSBench       | High-Performance Computing         | 14 GB   |
<!-- | Btree         | Measures index lookup performance  | 16 GB   |
| GUPS          | Random Memory Access Patterns      | 16 GB   | -->

### page tracking

页表扫描的核心目标是检测进程的内存访问行为，这通常通过在两次扫描之间检查页表项的 ACCESS 位（A 位）是否被置 1 来实现。具体而言，扫描过程中记录哪些页表项被访问，并在确认访问后清除 A 位，同时刷新对应的 TLB（Translation Lookaside Buffer）中的缓存项。这一机制不可避免地会引入额外的开销，主要体现在增加访存延迟和影响程序性能。

我们的分析表明，进程的活跃页表项数量与扫描所需时间呈正相关关系。扫描时间越长，意味着当前系统内存访问负载较高。如果仍然采用固定时间间隔进行扫描，可能会导致不必要的额外开销，尤其是在高负载情况下进一步加剧性能损耗。

![](./result/track/scan_time/scan_time.png)

在实验中，我们编写了一个测试程序，该程序分配 16GB 内存并进行持续的随机访存，并记录在1s内的访存次数，以此来评估启用内核模块进行后台页表扫描对系统性能的影响。实验中设置了不同的扫描间隔，分别为 1000ms、2000ms、4000ms 和 6000ms，旨在观察不同扫描频率下对访存性能的影响。

结果表明，后台进行页表扫描显著影响了程序的访存性能，可以看出扫描间隔越短对系统访存性能的影响越大，并且在扫描发生时系统的访存性能下降最为明显。当扫描间隔设置为 6000ms 时，即每 6 秒进行一次后台扫描，此时系统的访存次数显著减少。这是因为后台扫描在消耗 CPU 资源的同时，清除了 TLB 中的页表项缓存，从而导致每次访存时必须重新访问内存，进一步加重了内存访问的延迟。扫描结束后随着时间推移，系统的访存性能逐渐恢复到基线（baseline）水平。

通过这种测试，我们可以看出，启用后台页表扫描会影响程序的访存效率，尤其是在高频扫描的情况下。随着扫描频率的增加，系统资源被更多地分配给页表扫描过程，导致正常的内存访问延迟增加，最终影响整体性能。因此，在设计内存管理和页表扫描策略时，需要平衡性能需求和扫描频率，以避免过度干扰系统的正常运行。

![](./result/track/pt_scan_impact/memory_accesses.png)

我们提出了一种动态调整扫描间隔的方法，使其能够根据实际的内存访问活跃程度进行自适应调整。

具体地，我们通过如下公式来计算扫描间隔 T_interval：

T_interval = (SCAN_K * T_scan + SCAN_K_BASE) * T_scan + SCAN_BASE_INTERVAL

t = (ax+b)x + c

其中：  
- T_scan 代表当前扫描所消耗的时间  
- SCAN_K 和 SCAN_K_BASE 是调节系数，用于控制扫描间隔对扫描时间的敏感度  
- SCAN_BASE_INTERVAL 是基础扫描间隔，保证低负载情况下仍然能进行周期性扫描  

该动态调整策略能够根据系统负载适应性地延长或缩短扫描间隔，降低对程序性能的影响，同时保持对进程访存行为的有效监控。

overhead 测试了 500 1000 2000 4000

- SCAN_K = 0.5
- SCAN_K_BASE = 50
- SCAN_BASE_INTERVAL = 2000

2000+ 

![](./result/track/overhead/overhead.png)

优化的页表树扫描：在 Linux 内核中，针对进程页面的遍历通常使用 walk_page_vma 进行单个 VMA（虚拟内存区域）的页表遍历，而 walk_page_range 可以用于访问整个进程的地址空间。然而，传统的页表扫描方法存在性能瓶颈，特别是在大规模进程内存管理时。Linux 采用 四级/五级页表 结构进行地址翻译，页表访问的层次包括：PGD（Page Global Directory）PUD（Page Upper Directory）PMD（Page Middle Directory）PTE（Page Table Entry）（P4D，部分体系结构使用五级页表）

在进行一次完整的地址翻译时，硬件会依次访问每一层的页表项，并且 每一级的页表项都会被设置 ACCESS 位。这意味着如果某个 PTE 级别的页表项被访问，那么它的所有上级页表节点（PMD/PUD/PGD）的 ACCESS 位也会被置位。

基于这一观察，我们可以优化页表扫描的范围：

逐层检查 ACCESS 位：如果一个 父页表节点（PGD/PUD/PMD） 的 ACCESS 位未被设置，则可以直接跳过其所有子节点的扫描，因为它们必然未被访问。

减少不必要的 PTE 级别扫描：传统 walk_page_range 可能会访问整个进程的地址空间，而在许多情况下，未被访问的页表区域无需遍历。

借鉴 telescope 设计思路：telescope 通过优化 DAMON（Data Access Monitoring）采样来提高访存监控效率，我们借鉴其思想，优化页表扫描逻辑，使其适用于 更普适的页表扫描场景，不仅限于 DAMON 采样。

![](./result/track/pt_scan/pt_scan_redis_bar.png)

redis xsbench pr 都是访存比较密集的应用， graph500 的提升特别明显， 因为 Graph500 主要用于评测大规模图处理的性能，它的核心算法是 BFS（Breadth-First Search，广度优先搜索），Graph500 需要遍历大型图的数据结构（通常是邻接表或压缩格式存储），访问模式呈非连续、非局部特性, 这导致其访存模式是高度稀疏的

opt 的扫描模式会去除掉根节点的访问，大幅缩小扫描范围

![](./result/track/pt_scan/pt_scan_opt_all.png)

### page migration

## conclusion and future work


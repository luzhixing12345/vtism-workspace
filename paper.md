
# vtism

## introduction

在不断发展的计算领域，数据密集型应用，如人工智能、大数据分析和基于云的服务的指数级增长对内存系统提出了前所未有的要求。传统的单层内存架构主要依赖于动态随机存取内存（DRAM），很难跟上这些不断升级的要求。DRAM 的局限性，包括相对较高的每比特成本、有限的容量可扩展性和能源效率低下，已成为实现高性能和低成本计算解决方案的重大瓶颈。

为应对这些挑战，多层内存系统已成为一种前景广阔的替代方案。这些系统将不同类型的内存整合到一个分层结构中，每种内存在访问延迟、容量和成本方面都有不同的特点。在低层，快速但昂贵的 DRAM 可为常用数据提供低延迟访问，确保关键操作的快速响应时间。在中层或高层，非易失性内存（NVM）或其他新兴内存技术以较低的每比特成本提供较高的容量，尽管访问延迟较高。这种分层安排可以更有效地利用内存资源，平衡性能与成本，满足不同应用和数据访问模式的特定要求。

在多层内存系统的各种组件中，Compute Express Link（CXL）内存备受关注。CXL 是一种开放式标准互连协议，可实现对远程内存资源的直接和高速缓存一致性访问。该技术在现代计算中具有几个关键优势。首先，它能在 CPU 和内存设备之间提供高带宽通信，这对于需要快速数据传输的应用（如内存数据库和大规模数据处理）至关重要。其次，CXL 内存支持内存池和共享等功能，允许在多个计算节点之间更灵活地分配资源。这在虚拟化环境中尤其有益，因为在这种环境中，多个虚拟机（VM）需要高效地访问和共享内存资源。此外，CXL 的高速缓存一致性机制可确保不同内存层之间的数据一致性，从而降低内存管理的复杂性，提高系统的整体性能[3, 4]。

分层内存管理包括一系列旨在优化不同内存层利用率的技术和策略。页面放置是一个关键环节，它涉及根据每个页面的访问频率和数据特征，为其确定最合适的内存层。例如，访问频率高的热页面通常放置在 DRAM 中，以尽量减少访问延迟，而冷页面则可以迁移到延迟较高、成本较低的内存层。页面迁移是另一个重要的组成部分，可以随着应用访问模式的变化，在不同内存层之间移动页面。这有助于平衡整个内存层的负载，提高系统的整体性能。此外，内存管理还包括缓存管理、内存分配和去分配等考虑因素，以确保有效利用资源。

然而，在虚拟化环境中应用分层内存系统时，仍有几个难题尚未解决。其中一个主要问题是跨多个虚拟机和不同内存层管理高速缓存一致性的复杂性。在虚拟化环境中，多个虚拟机共享物理内存资源，保持缓存一致性变得更加复杂。要确保所有虚拟机对不同内存层中的数据拥有一致的视图，同时又不产生过多的开销，这是一个巨大的挑战。现有的缓存一致性协议往往难以在这种多租户环境中有效扩展，从而导致潜在的数据不一致和性能下降。

另一个挑战在于如何有效处理内存访问延迟差异。不同内存层的不同访问延迟会严重影响虚拟机的性能。由于在虚拟机上运行的应用程序具有不同的内存访问模式，因此很难预测和优化这些延迟变化。例如，一些应用程序从远程内存层访问数据时可能会出现大量缓存未命中，从而导致处理延迟增加。开发能够实时适应这些延迟差异并优化每个虚拟机内存访问的智能算法和技术至关重要，但这仍是一个有待解决的问题。

此外，虚拟化环境的动态特性（虚拟机的数量及其资源需求可能会迅速变化）也给高效的内存访问带来了挑战。

## background and motivation

现有的分层内存管理系统可以较好的平衡系统中异构内存之间的页面追踪, 分类和迁移, 但是在虚拟化场景下有一些特殊的挑战 因此需要专为虚拟化设计的分层内存管理系统

1. 分层内存管理的最终目的是充分利用异构内存的容量，性能的差异，合理分配和迁移页面使系统性能提高，直观表现体现在运行在操作系统中的进程性能得到提升。但是在虚拟化场景下，大部分云厂商选择利用一台物理服务器的CPU和内存创建许多个虚拟机供外部服务，在这个场景下虚拟机管理程序VMM作为中间层管理每一个虚拟机，虚拟化场景下的根本目标并不是利用分层内存管理使得VMM性能提高，而是使得VMM管理的每一个guest os的性能提高。guest os 并不直接受 host os 管理，它更像是一个 VMM 进程中的 OS 进程，因此，虚拟化场景下，分层内存管理系统的设计目标应该更加关注 guest os 的性能，而不是 VMM 的性能。

   因此需要精准定位 guest os 中的热点页面而不是 VMM 进程的热点页面

2. 在宿主机中,我们无法直接得知虚拟机中的真实页面类型,因为页面类型的判断是基于页面缓存(page cache)进行的,而这些信息仅由虚拟机的内核(guest kernel)掌握。宿主机通过扫描VMM进程页表的方式判断页面类型是不准确的,具体的页面类型和管理策略仍由虚拟机内部的操作系统负责

统计了 liblinear， 使用了 HIGGS 数据集训练，结果证明宿主机会将所有 VMM 进程的页面均视作匿名页面， 而在虚拟机中统计结果发现其实还有来自 HIGGS 的大量的文件页没有被正确识别，linux内核中对于文件页和匿名页的回收策略有所不同，通常对于文件页的回收要比匿名页更加激进，大部分现有的工作也都考虑到了文件页和匿名页的差异来决定回收策略

![](result/trace/host_guest_file_anon/anon_pages_comparison.png)

redis

![](./result/trace/gpt-vs-ept/gpt-vs-ept.png)


## vtism design and implementation

## evaluation and analysis

### Experimental Setup

| **Component**  | **Specification**  |
|---------------|------------------|
| **CPU**       | 2× Intel® Xeon® Platinum 8468V CPUs @3.8 GHz, 48 cores per socket, 192 total CPUs, Hyper-Threading disabled |
| **Memory**    | **NUMA Node 0 & 1:** DDR5-4800, 32GB per channel, Hynix DIMM modules <br> **NUMA Node 2 & 3:** CXL-DRAM, 64GB, Hynix DIMM modules |
| **NUMA Nodes** | Node 0: CPU 0-47, 32GB DDR5-4800 <br> Node 1: CPU 48-95, 32GB DDR5-4800 <br> Node 2: CXL-DRAM, 64GB <br> Node 3: CXL-DRAM, 64GB |
| **Virtualization** | VT-x support |         

- autonuma 
- tpp
- nomad

Hemem PEBS 虚拟化不合适

| **Benchmark**  | **Category**                         | **RSS**  |
|---------------|--------------------------------------|---------|
| Redis (YCSB)  | Key-Value Store                     | 29 GB   |
| PR            | Graph Processing                    | 21 GB   |
| Graph500      | Graph Processing                    | 15 GB   |
| XSBench       | High-Performance Computing         | 14 GB   |
<!-- | Btree         | Measures index lookup performance  | 16 GB   |
| GUPS          | Random Memory Access Patterns      | 16 GB   | -->

### page tracking

页表扫描的核心目标是检测进程的内存访问行为，这通常通过在两次扫描之间检查页表项的 ACCESS 位（A 位）是否被置 1 来实现。具体而言，扫描过程中记录哪些页表项被访问，并在确认访问后清除 A 位，同时刷新对应的 TLB（Translation Lookaside Buffer）中的缓存项。这一机制不可避免地会引入额外的开销，主要体现在增加访存延迟和影响程序性能。

我们的分析表明，进程的活跃页表项数量与扫描所需时间呈正相关关系。扫描时间越长，意味着当前系统内存访问负载较高。如果仍然采用固定时间间隔进行扫描，可能会导致不必要的额外开销，尤其是在高负载情况下进一步加剧性能损耗。

![](./result/trace/scan_time/scan_time.png)

在实验中，我们编写了一个测试程序，该程序分配 16GB 内存并进行持续的随机访存，并记录在1s内的访存次数，以此来评估启用内核模块进行后台页表扫描对系统性能的影响。实验中设置了不同的扫描间隔，分别为 1000ms、2000ms、4000ms 和 6000ms，旨在观察不同扫描频率下对访存性能的影响。

结果表明，后台进行页表扫描显著影响了程序的访存性能，可以看出扫描间隔越短对系统访存性能的影响越大，并且在扫描发生时系统的访存性能下降最为明显。当扫描间隔设置为 6000ms 时，即每 6 秒进行一次后台扫描，此时系统的访存次数显著减少。这是因为后台扫描在消耗 CPU 资源的同时，清除了 TLB 中的页表项缓存，从而导致每次访存时必须重新访问内存，进一步加重了内存访问的延迟。扫描结束后随着时间推移，系统的访存性能逐渐恢复到基线（baseline）水平。

通过这种测试，我们可以看出，启用后台页表扫描会影响程序的访存效率，尤其是在高频扫描的情况下。随着扫描频率的增加，系统资源被更多地分配给页表扫描过程，导致正常的内存访问延迟增加，最终影响整体性能。因此，在设计内存管理和页表扫描策略时，需要平衡性能需求和扫描频率，以避免过度干扰系统的正常运行。

![](./result/trace/pt_scan_impact/memory_accesses.png)

我们提出了一种动态调整扫描间隔的方法，使其能够根据实际的内存访问活跃程度进行自适应调整。

具体地，我们通过如下公式来计算扫描间隔 T_interval：

T_interval = (SCAN_K * T_scan + SCAN_K_BASE) * T_scan + SCAN_BASE_INTERVAL

t = (ax+b)x + c

其中：  
- T_scan 代表当前扫描所消耗的时间  
- SCAN_K 和 SCAN_K_BASE 是调节系数，用于控制扫描间隔对扫描时间的敏感度  
- SCAN_BASE_INTERVAL 是基础扫描间隔，保证低负载情况下仍然能进行周期性扫描  

该动态调整策略能够根据系统负载适应性地延长或缩短扫描间隔，降低对程序性能的影响，同时保持对进程访存行为的有效监控。

overhead 测试了 500 1000 2000 4000

- SCAN_K = 0.5
- SCAN_K_BASE = 50
- SCAN_BASE_INTERVAL = 2000

2000+ 

![](./result/trace/overhead/overhead.png)

优化的页表树扫描， linux内核四级/五级页表，telescope 只对于 DAMON 采样做了优化，walk_page_vma walk_page_vma_opt 如果上级页表项没有A直接返回，在benchmark中最多提升缩小近一半的扫描范围

![](./result/trace/pt_scan/pt_scan_redis_bar.png)

redis xsbench pr 都是访存比较密集的应用， graph500 的提升特别明显， 因为 Graph500 主要用于评测大规模图处理的性能，它的核心算法是 BFS（Breadth-First Search，广度优先搜索），Graph500 需要遍历大型图的数据结构（通常是邻接表或压缩格式存储），访问模式呈非连续、非局部特性, 这导致其访存模式是高度稀疏的

opt 的扫描模式会去除掉根节点的访问，大幅缩小扫描范围

![](./result/trace/pt_scan/pt_scan_opt_all.png)

## conclusion and future work


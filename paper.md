
# vtism

## 1 introduction

随着人工智能、大数据分析和云计算等内存密集型应用的快速发展，现代计算系统面临着日益严峻的内存资源挑战。这些应用不仅需要大容量内存支持，还对访问延迟和带宽提出了更高要求。虚拟机（VM）凭借其多租户隔离和资源管理能力，已成为部署这类应用的主流平台。

然而，传统DRAM内存系统正面临多重限制：（1）单位比特成本居高不下；（2）DDR接口技术发展趋缓，容量和带宽扩展受限；（3）内存通道数量受CPU封装和PCB设计约束。这些问题导致在云数据中心等多VM共享场景下，DRAM资源极易饱和，造成显著的性能下降。

为应对这些挑战，基于CXL协议的异构内存架构应运而生。该架构通过整合高性能DRAM与大容量、低成本CXL内存，构建了分层内存系统。CXL的关键优势包括：（1）支持缓存一致性的内存扩展；（2）突破传统内存通道限制；（3）降低总体拥有成本（TCO）。在虚拟化环境中，CXL内存可作为非CPU NUMA节点，通过一致性协议简化内存管理。

分层内存管理是一种优化异构内存系统性能的关键技术，其核心思想是通过构建由不同性能层级（如高速DRAM和大容量CXL内存）组成的内存架构，并基于数据访问模式实现智能化的资源调度。该技术主要包含三个关键机制:页面追踪机制,通过监控内存访问位和脏位状态，实时采集页面的活跃度信息。现代系统通常采用硬件辅助的轻量级监控方式，以降低性能开销。页面分类算法,基于采集的访问模式数据，运用改进的LRU（如MGLRU）等算法构建冷热页面模型。页面迁移，将低频访问的冷页面迁移至CXL等慢速内存节点，同时保持热点数据在本地NUMA节点的DRAM中。从而在不牺牲性能的前提下提升整体内存容量与运行效率·


虚拟化环境中的分层内存管理面临诸多独特挑战。现有分层内存管理方案 [nimble, autonuma, memtis, nomad, hemem, colloid] 虽然在原生系统中表现良好，但在虚拟化场景下面临一系列新的挑战与约束。虚拟化环境存在显著的语义隔阂问题[2.1]。由于Guest OS的物理内存需要通过VMM映射到主机物理内存，导致宿主机无法准确识别Guest OS内部的实际工作负载和页面类型（文件页/匿名页），进而引发页面追踪范围扩大，内存回收策略失配等问题。

在页面追踪方面，传统方法在虚拟化环境中面临严重性能瓶颈。线性页表扫描[thermostat]存在过大开销，基于页错误[autonuma,tpp,nomad]的方法会引发频繁VM Exit，而硬件计数器技术（如PEBS）[memtis,hemem]又受限于虚拟化环境下的访问权限。这些限制使得现有追踪机制难以满足虚拟化环境对精确性和低开销的双重要求。

与此同时，虚拟化环境也带来了区别于原生系统的优化契机。以 KVM 为代表的 Type-1 Hypervisor 与 Linux 内核实现深度集成，使得 Host 内核能够通过内存管理机制跨越 VMM 层对虚拟机进行统一调控。在硬件辅助的EPT/NPT技术的帮助下，Hypervisor 只需修改EPT中的映射关系即可完全独立地控制GPA到HPA的高效二维映射，为页面迁移提供了硬件基础

虚拟化环境还具备物理内存共享的天然优势，为 Host 与 Guest 之间的协同优化提供了更高效的路径。这使得二者可以轻易的通过共享内存而非网络传输进行低延迟的页面热度信息的传递。

此外，公有云环境中资源复用的实际部署需求进一步放大了虚拟化内存优化的重要性。例如在 Azure 等云平台中，一台物理主机往往需承载数十个虚拟机实例。然而实际观测表明，大多数虚拟机长时间处于低负载状态，其分配的大量内存资源处于闲置或低效使用状态。这种“资源过配”问题使得基于虚拟机实时内存访问模式进行动态优化成为可能且必要。通过感知虚拟机的访问热度特征并进行页面分类与动态迁移，可以在保证服务性能的前提下提升整体内存利用率，有效降低 TCO（Total Cost of Ownership），并增强内存资源的弹性调度能力。

本文提出一种虚拟化感知的分层内存管理框架 vtism，其核心贡献包括：

- 自适应的客户端页面追踪，可以根据工作负载实时调整扫描强度
- 基于 MGLRU 的 Guest-Host协同的页面热度和页面类型反馈策略，适应各种内存访问模式；
- NUMA 感知的异步并行页面迁移，结合 NUMA 节点信息综合考虑迁移位置，并分离页面提升和降级逻辑

实验结果表明，该方案在保持低于5%性能开销的同时，可提升内存利用率达40%，显著优于现有方案。代码和测试结果全部开源,可以在 [URL]() 找到

<!-- (3) 多租户环境中的内存利用率优化
在云计算环境中,运行虚拟机的服务器通常只运行多个虚拟机,而不会运行其他的本地服务.每个虚拟机的内存和 CPU 资源分配相对有限,例如 8 vCPUs 和 32GB 内存(及以下配置)为主流.因此,单个虚拟机通常无法充分利用物理主机上的所有计算和内存资源.

研究表明 [16],在 Azure 的典型计算集群中:40% 的虚拟机使用不超过 2 个 vCPU,86% 的虚拟机使用不超过 8 个 vCPU.

这意味着现代云主机通常需要同时运行数十个虚拟机,但大多数虚拟机的实际负载较低,导致其分配的内存资源可能会出现闲置.传统的 Hypervisor 资源管理策略主要依赖于静态或准动态的内存分配,难以实时适应虚拟机的内存需求波动.

然而,在分层内存管理框架下,可以基于虚拟机的实时内存访问模式进行动态优化:对于长时间未被访问的冷数据,可将其降级到 CXL DRAM 或 SSD Swap,以释放高性能 DDR DRAM 资源. -->

## 2 background and motivation

key insight

### 2.1 虚拟化的语义隔阂

虚拟化环境中，宿主机由于存在虚拟机管理器（VMM）这一中间抽象层，无法直接感知 Guest 虚拟机中实际运行的应用负载及其内存使用行为。这种“语义隔阂”严重制约了宿主机对内存管理策略的精细化调度能力，特别体现在页面追踪与页面分类等核心任务上。

我们设计了一组实验来量化宿主机与虚拟机在内存追踪粒度上的差异. 在一台 配置 32GB 内存的 VM 上运行 Redis + YCSB 负载,评估了 HPT 扫描(Host 侧扫描)与 GPT 扫描(Guest 侧扫描)在扫描范围上的差异.运行前对 VM 进行预热,建立完整的 32GB 内存映射. 实验结果表明 由于宿主机无法感知 Guest OS 进程的工作负载情况,始终扫描完整的 32GB 内存范围.Guest OS 直接感知 进程实际使用的内存区域,仅扫描真实工作负载,大大减少了要扫描的页面数量

![](./result/track/gpt-vs-hpt/gpt-vs-hpt.png)

此外，在虚拟化环境中，宿主机无法直接获取虚拟机内部页面的真实类型信息，这是因为页面类型的判定依赖于 Guest OS 内核中的页缓存（Page Cache）及其管理策略，这些信息完全被封装在虚拟机内部。当前常见的一种做法是通过扫描 VMM（如 QEMU）进程的页表并通过 page cache 获取页面类型，但由于 VMM 仅作为 Guest OS 的抽象表示，其宿主机侧的页面并不能反映实际应用的页面使用语义，因此这种方法在准确性上存在显著不足。

我们使用 liblinear 进行测试, 其使用了一个包含 7.4GB HIGGS 数据集的大文件,并在宿主机与虚拟机内部分别统计页面类型, 具体的实验配置细节见 [4]。实验结果表明宿主机将 VMM 进程所分配的所有页面均标识为匿名页（Anonymous Pages）；而虚拟机内部则能够准确区分出对应于 HIGGS 数据集的文件页（File-backed Pages）与匿名页。此外宿主机还统计出了约 1GB 属于 QEMU 本身管理结构的内存.这表明宿主机对虚拟机页面类型识别能力的严重缺失。

![](result/track/host_guest_file_anon/anon_pages_comparison.png)

Linux 内核对文件页(File-backed Pages) 和 匿名页(Anonymous Pages) 采用 不同的回收策略:文件页 由于有后备存储(如磁盘),通常 更容易被回收,策略也 更激进;匿名页(如应用程序的堆、栈)没有后备存储,回收时需要进行 交换(Swap-out),因此回收更为 保守.

虚拟化环境中的语义隔阂显著限制了宿主机对虚拟机内存使用行为的精准理解与管理，尤其在页面追踪与页面分类方面暴露出识别范围不准、页面类型误判等问题，导致回收策略准确性不足。要有效提升内存资源的使用效率，需要建立 Guest OS 与宿主机之间的协同机制，打破虚拟化带来的语义隔阂。

### 2.2 页表扫描间隔与活跃工作负载

<!-- vtmm 1min, Thermostat 30s -->

传统的页表扫描策略通常采用固定的扫描间隔，该间隔通过在不同 benchmark 负载下测试系统性能开销来设定，通常以确保在引入后台页表扫描模块后，前台进程的性能损失不超过 5% 为原则。我们认为这种固定间隔扫描方式存在显著的局限性，不同虚拟机实例所承载的应用具有高度异质的内存访问模式与工作集动态行为，其访存活跃程度与工作负载强度差异显著，单一固定扫描间隔难以同时兼顾不同类型的虚拟机应用。对于内存活跃度较低的轻量级应用，可能因扫描频率过低而错失关键页面冷热状态的变化，影响内存分层策略的准确性；而对于工作集频繁变化的高负载应用，则扫描频率过高会引入不必要的开销

一次页表扫描由扫描时间和扫描间隔两部分构成. 扫描间隔的设置是为了缓解由于页表扫描带来的性能开销, 而扫描时间与进程的活跃页表项数量直接相关. 我们在虚拟机中运行了 gabps pr,并记录后台页表扫描时的活跃页表项和扫描时间的变化曲线; figure 测试了不同活跃工作负载下对应的扫描时间,结果表明,进程的活跃页表项数量与扫描时间呈正相关,每 1GB 工作负载对应 4-5ms 后台扫描时间.当前系统的活跃内存访问负载越高, 意味着扫描时间越长.

![](./result/track/scan_time/pr_scan_time.png)

![](./result/track/scan_time/scan_time.png)

页表扫描过程中重新设置 A/D 位和刷新 TLB 会显著降低 guest os 内存访问速度,在进行两级地址转换的虚拟化环境中带来的开销尤为明显.在实验中,我们编写了一个测试程序,该程序分配 16GB 内存并进行持续的随机访存,并记录1s内的总访存次数,以此来评估启用内核模块进行后台页表扫描对系统性能的影响.实验中保持相同的活跃工作负载以维持扫描时间相同，对比设置不同的扫描间隔,分别为 1000ms、2000ms、4000ms 和 6000ms,旨在观察不同扫描频率下对访存性能的影响.

我们将测试程序和页表扫描模块在虚拟机中运行,结果表明,后台进行页表扫描显著影响了程序的访存性能, 扫描间隔越短对系统访存性能的影响越大,并且在扫描发生时系统的访存性能下降最为明显.当扫描间隔设置为 6000ms 时,即每 6 秒进行一次后台扫描,此时系统的访存次数显著减少.这是因为后台扫描在消耗 CPU 资源的同时,清除了 TLB 中的页表项缓存,从而导致每次访存时必须重新访问内存,进一步加重了内存访问的延迟.一轮扫描结束后，系统的访存性能逐渐恢复到基线(baseline)水平.

![](./result/track/pt_scan_impact/memory_accesses.png)

我们观察到，页表扫描对不同负载程序的性能影响呈现出非线性特征。我们设计了一组实验,通过在测试程序中循环访问内存区域的所有页面,并以固定的扫描间隔进行页表扫描,以此评估不同规模的工作负载对进程性能的影响.Y 轴表示在后台开启页表扫描的情况下程序的运行时间减速比例.实验结果表明,随着工作负载增加,程序运行时间的减速幅度并非线性增长,而是近似二次方增长，且扫描间隔越长相对影响越小。

![](./result/track/xx_proof/multi_interval_slowdown.png)

在更大的活跃工作集下，页表扫描需遍历的页表项数量显著增加，导致单轮扫描时间增长。而扫描过程中的一系列操作——包括访问页表结构、修改页表项中的 A/D 位以及刷新对应的 TLB 项——对系统性能的影响并非线性叠加。修改 A/D 位需持有页表项对应的 spinlock，而大范围的 TLB 刷新会破坏处理器高速缓存一致性，从而加剧前台程序的访存延迟。这些因素共同作用，导致页表扫描对进程执行时间的影响随工作负载扩大而非线性放大。

我们观察到，当页表扫描间隔设置为1000ms和2000ms时，程序运行时间的减速比分别在活跃工作集达到约8GB和16GB后趋于饱和，最终稳定在约5倍的减速上限，这一现象的根本原因在于页表扫描的周期性执行机制与程序内存访问模式之间的相互作用。

在较短的扫描间隔（如1000ms）下，即使其总分配并访问内存较大（如10GB），程序在两次扫描之间的时间窗口内也仅能访问部分活跃页面（如8GB），而随后的页表扫描会重置这些页面的 A/D位，导致扫描范围被限制在该时间窗口内可访问的页面范围内。类似地，当扫描间隔延长至2000ms时，程序可访问的页面范围扩大至约16GB，但超出该范围的工作集不会进一步增加扫描开销，因为后续扫描仍仅覆盖该时间窗口内被触及的页面。因此，当活跃工作集超过扫描间隔所对应的最大可访问内存量时，页表扫描引入的性能干扰不再随工作集增大而显著增加，从而使得减速比趋于稳定。

通过实验我们得到两个结论(1) 系统的活跃内存访问负载越高, 扫描时间越长，二者成正相关 (2) 扫描时间非线性影响程序访存性能。因此需要根据实时工作负载情况动态平衡性能需求和扫描频率，如果仍然采用固定时间间隔进行扫描,可能难以适应不同规模工作集的性能敏感性，需引入更为灵活的扫描间隔控制策略以缓解性能开销

## 3 vtism design and implementation

### 3.1 overview

vtism 是一个专为 虚拟化环境 设计的 分层内存管理系统, 它遵循页面跟踪、分类和迁移的经典流程，但在每个环节都引入了创新性设计，充分利用 硬件虚拟化特性(如 EPT)和 软件优化(MGLRU), 通过减少 Guest OS 与 Host OS 之间的内存管理语义隔阂,优化 跨 NUMA 低延迟的页面提升(Promotion)与降级(Demotion),确保充分发挥虚拟机在分层内存系统上的性能潜力

vtism 包含三大核心模块

Adaptive Optimized Guest Page tracking：在虚拟机内部追踪其内存访问模式, 并根据活跃负载动态调整扫描频率, 优化 A/D 扫描范围,使用共享内存向宿主机内核暴露guest os进程级工作负载信息.

MGLRU增强的页面温度分类器（MGLRU-Enhanced Page Temperature Classifier）: 使用 guest 侧的页面访问信息代替 MGLRU 在 host 侧的主动扫描，结合Guest侧页面访问次数和页面类型的提示和Host侧的观测, 形成对不同页面类型更精准的冷热分类, 主动推动 MGLRU 迭代提高页面提升和页面回收决策精准度.

NUMA优化的异步页面迁移器（NUMA-Aware Asynchronous Page Migrator）: 在每个 NUMA 节点分别维护一对页面提升与降级队列，根据迁移类型的源节点和目标节点将迁移任务调度到最佳节点，在保持NUMA亲和性的前提下最小化迁移延迟. 并根据 NUMA 链路延迟实时调整迁移方向，迁移后主动修改扩展页表 （EPT）的新映射，以避免 EPT 页面错误导致的 VMTraps

### 3.2 Adaptive Optimized Guest Page tracking

#### 3.2.1 guest page table tracking

2.1 节提到虚拟化环境存在语义隔阂，由于 VMM 抽象层，宿主机无法直接感知 Guest 虚拟机中实际运行的应用负载及其内存使用行为, 主要表现在 (1) 宿主机没有能力跨越 VMM 中间层感知 guest VM 上实际运行的工作负载大小.因此宿主机需要扫描整个 VM 的 全部映射内存以判断页面访问情况,导致扫描时间过长 (2) host 会将 VM 的所有页面视为匿名页，无法跨越 VMM 感知 Guest OS 内核中的真实的页缓存类型，导致页面回收策略准确性不足

在虚拟化环境下在宿主机中传统的页表扫描会带来额外的开销，现代 VMM 通常采用硬件虚拟化的两级地址转换(EPT/NPT),即 Guest 虚拟地址(GVA)→ Guest 物理地址(GPA)→ Host 物理地址(HPA), 重新设置 A/D 位和刷新 TLB 会显著降低 guest os 内存访问速度,带来昂贵的访存开销

vtism 采用 Guest Page Table(GPT)扫描,在 Guest OS 内部嵌入一个内核扫描模块,替代传统的宿主机 HPT(Host Page Table)扫描方式. 因为 Guest OS 具备对自身进程的完整感知能力,可以精准扫描实际使用的内存页,而无需扫描整个 VM 的全部映射内存.

#### 3.2.2 Shared Memory

在虚拟机创建阶段，vtism 利用 Inter-VM Shared Memory（ivshmem） 设备机制，在 Host 与 Guest 之间建立一段高速、零拷贝的共享内存区域。该共享内存通过 PCI BAR 映射方式呈现在 Guest 内核空间，并由 Host 内核以共享内存的形式接收。

在 Guest 内部，vtism 实现了一个专用的字符设备驱动 ivshmem_dev，用于管理该共享内存区域的读写。该驱动负责将 Guest 页表扫描模块收集到的进程级访问模式与页面热度信息结构化后，直接写入映射到共享内存的物理地址空间。写入行为无需系统调用上下文切换或中断通知，极大降低了信息同步的延迟和开销。Host 侧内核模块则通过固定的虚拟地址访问该共享内存区域，并以零拷贝方式解析 Guest 提供的页表扫描数据

对于一个 32GB 的 VM，以 4KB 分割页面，每个页表项需要 8 bytes size 记录，共需要 64MB 共享内存区域。VM 容量和共享内存区域的大小比为 512:1. 我们在共享内存开头写入控制信息，包括扫描轮次，页表项条目数，扫描间隔，以便 host 和 guest 同步。

#### 3.2.3 Adaptive scan interval

vtism 提出了一种动态自适应间隔的页表扫描策略, 使其能够根据实际的内存访问活跃程度进行自适应调整.

具体地,我们通过如下公式来计算扫描间隔 T_interval, 下一轮扫描间隔 T_interval 会随扫描时间 Tscan 的变化而变化以适应不同的内存访问模式:

T_interval = (SCAN_K * T_scan + SCAN_K_BASE) * T_scan + SCAN_BASE_INTERVAL

其中:  
- T_scan 代表本轮扫描所消耗的时间  
- SCAN_K 和 SCAN_K_BASE 是调节系数,用于控制扫描间隔对扫描时间的敏感度  
- SCAN_BASE_INTERVAL 是基础扫描间隔,保证低负载情况下仍然能进行周期性扫描 

注意到扫描间隔的修正公式选择引入以 T_interval ∝ T_scan² 的非线性策略，这是因为 2.2 节我们通过实验验证发现扫描时间对程序性能的减速并非成线性关系,而是二次方关系. 后台的扫描时间 T_scan 对前台应用性能的影响会随着 T_scan 的增长呈加速式放大, 若采用线性调节策略（例如 T_interval = k × T_scan + b），无法有效抵消 T_scan 带来的非线性性能开销，在高负载场景下系统仍面临显著的性能退化。为此，我们引入以 T_interval ∝ T_scan² 的非线性策略，实现对页表扫描的性能开销的指数级抑制.

该动态调整策略能够自适应系统内存负载,平衡不同规模应用的影响,确保小内存应用不浪费 CPU 资源, 大内存应用不过度占用 CPU 资源. 不论 VM 中工作负载的访问模式如何变化始终可以保持对前台进程的性能影响不超过 5%，实验结果见 [4.3.1]

#### 3.2.4 Page Table scan optimization

Linux 采用四级/五级页表结构进行地址翻译,页表访问的层次包括:PGD(Page Global Directory)、PUD(Page Upper Directory)、PMD(Page Middle Directory)和 PTE(Page Table Entry)(部分架构在此基础上增加 P4D 形成五级页表).在内核中,针对进程页面的遍历通常使用 walk_page_vma 进行单个 VMA(虚拟内存区域)的页表遍历,或者使用 walk_page_range 访问整个进程的某一段地址空间.目前,大多数依赖页表扫描的系统(如 AutoNUMA、DAMON)直接使用 walk_page_range 遍历所有 PTE,以检查 A/D 位的设置情况.然而,这种方法存在显著的性能瓶颈,尤其是在 大规模内存环境 和 稀疏页表 场景下.

在 x86 以及其他支持 A/D 位的体系架构中,处理器在完成地址翻译时,会依次访问每一级页表项(PGD → PUD → PMD → PTE),并自动设置对应的 ACCESS 位(A 位).这意味着如果某个 PTE 级别的页表项被访问,那么它的所有上级页表节点(PGD/PUD/PMD)的 A 位也会被置 1.因此,如果在扫描过程中发现某个 父级页表节点的 A 位未被设置,可以直接跳过其所有子页表项的扫描,因为这些页表项必然未被访问.这一优化思路可以 大幅减少 PTE 级别的遍历,避免不必要的页表扫描,降低 CPU 负担.

基于这一观察,我们设计并实现了 优化版页表扫描函数 walk_page_vma_opt,通过 逐层检查 ACCESS 位 来 剪枝未被访问的页表节点,减少 PTE 级别的遍历开销.使用 walk_page_range 会访问整个进程的地址空间,而在许多情况下,未被访问的页表区域无需遍历.linux 内核 API 没有采用这个优化是因为 walk_page_range 的设计目的就是遍历进程的所有页表空间并进行对应的处理,这个优化操作是利用处理器硬件特性并专门针对页面A/D位扫描需求实现的. 该patch仅涉及到一个新文件的添加,可以被无缝应用到几乎大部分内核版本中

优化实验结果见 [4.3.2]

### 3.3 MGLRU-Enhanced Page Temperature Classifier

#### 3.3.1 Enhance Host MGLRU Classifier

vtism 对 MGLRU 进行了增强，设计并实现了一套 Guest-Host 协同的页面分类机制，突破了传统 MGLRU 仅依赖 Host 侧访问观测结果的限制。Guest OS 会定期将访问活跃的 GPA 写入共享内存区域, vtism 通过 KVM 模块解析 VM 的 EPT 映射，将 gpa 地址转换为对应的 hva，将 VM 的 gpa 地址转化为 host kernel 可以处理的 hva 地址。随后 Host 内核会将 VM 热点页面的访问信息和正确的页面类型追加至 MGLRU 的多代访问统计框架中，并主动推进 MGLRU 执行新一轮迭代，从而在多代队列结构中及时提升活跃页面至最新一代，避免其被错误回收。

此外，vtism 针对当前驻留于远程 NUMA 节点或 CXL 内存节点的活跃页面，能够基于页面热度主动发起页面提升操作，将其迁移至本地 NUMA 节点，以优化内存访问路径

#### 3.3.2 why use MGLRU

MGLRU（Multi-Generational Least Recently Used）是 Linux 社区提出的一种替代传统 LRU 算法的页面回收机制，其核心思想是通过引入“世代（generation）”的概念，将页面按访问时间划分为多个代队列，从而更加细粒度地描述页面的访问热度。这一机制在 Linux 6.1 内核中被主线采纳，已成为新一代内存回收策略的重要实现。已有大量的性能测试表明 MGLRU 在许多方面都显著优于传统 LRU 机制[引用]

传统 LRU 在匿名页与文件页之间缺乏统一的热度衡量方式，尤其在多 cgroup 场景下易出现冷页滞留或热页误回收等问题。MGLRU 通过统一的多代结构，使不同类型页面在全局范围内可比较，有效提升回收决策的准确性

此外，MGLRU 引入基于 refault 事件的自校正机制，当被回收页面短时间内重新被访问，系统会调整回收权重。配合 PID 控制器动态调节各代页面的回收频率，使得回收策略更加鲁棒，适应性更强。

### 3.4 NUMA-Aware Asynchronous Page Migration

<!-- 页面迁移的性能高度依赖于 NUMA 拓扑结构及其访问延迟特征。为了在保持 NUMA 亲和性的前提下最大化迁移效率，vtism 设计了一个 NUMA 感知的异步页面迁移器，结合工作队列机制、页面迁移流程异步解耦和实时延迟感知优化，实现对页面提升与降级操作的高效调度 -->

vtism 基于 Linux 内核的工作队列机制实现了异步页面迁移。每个 NUMA 节点上分别维护一对页面提升队列（promote_wq）与降级队列（demote_wq），用于处理跨节点的热页上迁与冷页下迁任务。迁移任务以工作项形式异步提交，由绑定目标节点 CPU 的内核线程在后台执行。该设计具有三点优势：

- 迁移队列本地化，减少跨节点同步与调度开销；
- 任务类型分离，将提升队列和降级队列分离，避免多节点竞争锁导致性能退化；
- 优先级差异化处理：页面提升任务具有实时性要求，为保障关键数据的低延迟访问，promote_wq 被配置为 WQ_HIGHPRI 高优先级队列，使得热页迁移可以优先完成。而页面降级操作本质为后台内存整理任务，对实时性要求较低，demote_wq 可延迟调度，由内核在对应 NUMA 节点 CPU 空闲时执行，以减小系统扰动。

在页面提升时，迁移任务会被提交到目标 NUMA 节点（即目标页所在节点）的 promote_wq，以便目标节点线程完成执行迁移任务后提前预取迁移目标页面的内容，降低迁移完成后的首次访问延迟，减少 TLB miss

在页面降级时，迁移任务则被提交至源 NUMA 节点的 demote_wq。这是因为源节点线程更接近原始页所在内存及其访问上下文，能够更高效地完成页面复制和页表更新。
<!-- 
#### 3.4.2 延迟感知的动态调度优化

在异构内存架构中,传统的分层内存管理系统通常基于静态的性能假设进行设计,即简单地认为DRAM节点的访问性能在所有场景下都优于CXL内存节点.然而,这种假设忽略了现代计算系统中内存访问行为的动态特性,特别是在高并发、多租户的虚拟化环境中.VTISM通过深入分析内存子系统的实际运行特征,提出了更精细化的迁移决策机制.

DRAM节点的访问延迟并非固定不变,而是会随着本地内存访问频率的增加呈现显著的非线性增长:实验测试了 1-32GB 工作负载持续访存,1channel 32GB DDR DRAM 访问延迟变化曲线.33ns->600ns, 本地 CXL DRAM 访存延迟 288 ns,远程 CXL DRAM 访存延迟 486 ns,随着工作负载的增加 DDR DRAM 的延迟迅速超过了 CXL DRAM 空载的延迟.本地不一定比远端好

![](./result/migration/latency/dram_latency.png)

VTISM采用智能化的迁移决策机制,在进行页面迁移前会对目标NUMA节点的访问特性进行综合评估.具体而言,系统会计算以下关键指标:

远程访问延迟代价:通过对比本地访问延迟与跨NUMA节点访问延迟,如果由于本地的高频访问导致远程访问延迟低于本地访问,那么此时并不

#### 3.4.2 实时延迟获取

UNC_CHA_TOR_OCCUPANCY.IA

UNC_CHA_TOR_INSERTS.IA

#### 3.4.3 异步多线程

Linux 的页面迁移有同步迁移和异步迁移两种模式,同步迁移先尝试使用MIGRATE_ASYNC模式批量迁移folios

如果批量迁移失败,则退回到逐个同步迁移每个folio [code](https://github.com/torvalds/linux/blob/ffc253263a1375a65fa6c9f62a893e9767fbebfa/mm/migrate.c#L1825).

但是我们注意到现在 linux 内核使用的异步迁移不是真正的异步操作,依然是同步顺序的 copy 页面并在循环中调用 cond_resched[code](https://github.com/torvalds/linux/blob/ffc253263a1375a65fa6c9f62a893e9767fbebfa/mm/util.c#L790C1-L801C2)

多线程异步迁移,针对 页面提升(hot page migration) 和 页面降级(cold page demotion) 进行优化.思路是使用 waitqueue 进行异步提交,并在后台完成页面迁移.

在内核中,页面迁移(Page Migration)发生的原因多种多样,不同的迁移场景对执行顺序和阻塞行为有不同的要求.其中,一些页面迁移操作需要保持严格的执行顺序,因此必须同步进行并阻塞相关进程.例如内存规整(MR_COMPACTION),内存热插拔(MR_MEMORY_HOTPLUG)系统调用(MR_SYSCALL),调用 mbind 系统调用设置 memory policy 时触发的迁移(MR_MEMPOLICY_MBIND)等

另一方面,某些迁移类型主要用于内核在系统层面优化内存访问模式,因而无需同步执行,可以采用异步机制在后台完成,以减少对前台任务的影响.例如页面提升(MR_NUMA_MISPLACED)和页面降级 (MR_DEMOTION). 对于这些无需严格同步的迁移任务,内核可以采用异步方式进行后台迁移,以避免影响前台应用程序的执行效率.这种异步迁移策略可以提高系统的整体性能,同时保证内存资源的高效利用.

页面提升和页面降级的特性不同,因此可以采用 不同的队列调度策略

页面提升的目标是快速迁移高频访问的页到更快的存储(如 DRAM),先检查 目标 NUMA node 是否有足够的可用内存,避免迁移后导致回退.


页面降级的目标是迁移低访问频率的页面到较慢的存储. 低优先级的后台线程可以在内存压力较低时异步迁移,避免影响前台进程性能,采用 时间窗口(time window)+ 访问统计,避免短期冷热变化导致频繁迁移(MGLRU)

采用提升和降级双工作队列,不同的优先级任务交给不同的 worker.高优先级队列:处理页面提升,尽快将热点页面迁移到 DRAM,设置WQ_HIGHPRI:让热页迁移尽快完成,减少性能抖动;低优先级队列:处理页面降级,在 内存压力较低 时进行,设置WQ_UNBOUND:让调度更灵活,充分利用多核 CPU 资源.

减少锁争用:使用 RCU 避免页表锁,避免高并发时 mmap_lock 带来的性能瓶颈;冷页迁移可以使用 trylock,减少锁等待对应用程序的影响



活跃进程对其活跃页面的访问是影响系统性能的关键因素.如果这些活跃页面位于慢速内存节点(如 CXL-DRAM 或 NVM),我们希望尽快将其迁移到高速内存节点(如 DRAM),以降低访问延迟.然而,页面迁移的前提是目标节点具备足够的可用内存,否则迁移操作将会失败,从而影响性能优化的效果.因此,在执行页面迁移时,确保目标节点的可用内存充足是至关重要的.

为了解决这一问题,内核采用 空闲页面回收水位线(Watermark for Page Reclaim) 机制来决定何时启动页面回收.当系统空闲内存下降到设定的水位线以下时,内核会触发 页面降级(Page Demotion),即将 DRAM 中的冷页面迁移到 CXL-DRAM 或 NVM 等较慢的存储介质,以释放更多的 DRAM 资源.这一机制确保了当高优先级进程需要更多高速内存时,系统可以及时提供可用的 DRAM 页面,进而提高页面提升(Page Promotion)的成功率和效率.

由于内存回收和页面降级涉及大量的数据迁移,其执行需要一定的时间.因此,系统必须提前预留足够的空闲内存空间,以保障后续页面提升操作能够顺利进行.换句话说,提高 页面提升的成功率和执行效率 是提升系统性能的核心目标,而更主动地进行 提前页面降级 则是实现这一目标的重要保障.通过合理调整页面回收策略,使降级与提升协同工作,可以有效优化内存资源的分配,提高整体系统性能.

将内存页面从源位置迁移到目的位置需要四步:
(1)在目标节点中分配新页面
 (2) 取消映射要迁移的页面(包括使PTE失效)
 (3) 将页面从源节点复制到目标节点
(4)映射新页面(包括更新PTE)

> (1) allocate new pages in the target node; (2) unmap pages to migrate (including invalidating PTE); (3) copy pages from source to target node; (4) map new pages (including updating PTE)

每个节点一个 promote_wq demote_wq, wq numa node CPU 亲和性

https://docs.kernel.org/core-api/workqueue.html

promote_wq[dst_node]: 页面提升时,任务提交到 dst 所在 NUMA 节点的 promote_wq

demote_wq[src_node]: 页面降级时,任务提交到 dst 所在 NUMA 节点的 demote_wq

需要一个理由

nomad 只使用一个工作队列 [code](https://github.com/lingfenghsiang/Nomad/blob/602da42dcd6b6cf86ecfa50ba1ab7ab3b6dceb9f/src/nomad_module/async_promote_main.c#L1080) -->

## 4 evaluation and analysis

### 4.1 Experimental Setup

我们在一台双路服务器上构建了具有异构分层内存结构的实验平台，用于系统性评估 vtism 的性能表现。该平台配备了两颗 Intel Xeon Platinum 8468V 处理器，提供总计 96 个物理核心，Hyper-Threading 被关闭以消除 SMT 干扰。每个处理器连接一个 DDR5 内存节点（NUMA Node 0 和 Node 1），同时系统还集成了两个基于 Compute Express Link（CXL）的远程内存节点（NUMA Node 2 和 Node 3），使用 Hynix CXL-DRAM 模组，每个容量为 64GB，从而构建了典型的 DRAM-CXL 分层内存架构。CXL 设备连接在 Node1 上，与 node1 的 numa 拓扑比 node0 更近。

测试平台的详细硬件与软件配置如表所示[link]。所有实验均在运行 Linux 内核版本 v6.6 的 Ubuntu 22.04 LTS 上进行，确保内核对 MGLRU、CXL 以及虚拟化子系统的最新支持。虚拟机通过 QEMU 8.2.2 启动，分配 16 个 vCPU 和 32GB 内存，启用 KVM 虚拟化加速与 VT-x 指令集扩展。

在性能评估中，我们将 vtism 与当前主流的分层内存管理方案进行对比，包括 AutoNUMA、tpp、nomad

| **Component**       | **Specification**                                                                                                                 |
| ------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| **CPU**             | 2× Intel® Xeon® Platinum 8468V CPUs @3.8 GHz, 48 cores per socket, 192 total CPUs, Hyper-Threading disabled                       |
| **Memory**          | **NUMA Node 0 & 1:** DDR5-4800, 32GB per channel, Hynix DIMM modules <br> **NUMA Node 2 & 3:** CXL-DRAM, 64GB, Hynix DIMM modules |
| **NUMA Nodes**      | Node 0: CPU 0-47, 32GB DDR5-4800 <br> Node 1: CPU 48-95, 32GB DDR5-4800 <br> Node 2: CXL-DRAM, 64GB <br> Node 3: CXL-DRAM, 64GB   |
| **Virtualization**  | QEMU emulator version 8.2.2, 32GB VM, 16 CPU cores, KVM enabled, VT-x support                                                     |
| **System Software** | Ubuntu 22.04 LTS                                                                                                                  |


为全面覆盖不同负载类型，我们选取了六个广泛使用的 benchmark 工作负载，涵盖键值存储、图处理、机器学习、高性能计算及随机访问行为等多个类别，

- Redis (YCSB)：采用 Yahoo Cloud Serving Benchmark（YCSB）对 Redis 进行测试，模拟典型的云服务键值存储访问模式，反映高吞吐量、小数据对象的读写负载特征。
- PageRank (gabps)：基于 GAP Benchmark Suite 实现的 PageRank 算法，代表典型的迭代式图处理工作负载，具有频繁的稀疏数据访问和邻接访问模式。
- Graph500：国际图处理性能评估标准，用于测试大规模图遍历与搜索算法的内存子系统性能，强调带宽敏感性和不规则访问。
- Liblinear (HIGGS)：使用 HIGGS 数据集在 Liblinear 上进行训练，反映机器学习模型在训练过程中对大量特征数据的密集内存访问行为。
- XSBench：一个用于核物理模拟的内存密集型基准程序，具有高并发、以缓存为主的查询特征，代表高性能计算场景中对随机小对象的频繁访问。
- GUPS：Giga Updates Per Second，用于衡量系统在随机内存访问场景下的更新能力，是测试系统内存访问延迟和带宽的经典负载。

所属类别和 RSS 占用见表格

| **Benchmark**   | **Category**                      | **RSS** |
| --------------- | --------------------------------- | ------- |
| Redis (YCSB)    | Key-Value Store                   | 29 GB   |
| PageRank(gabps) | Graph Processing                  | 21 GB   |
| Graph500        | Graph Processing                  | 15 GB   |
| Liblinear(HIGGS)       | Machine learning                  | 10 GB   |
| XSBench         | High-Performance Computing        | 14 GB   |
| GUPS            | Random Memory Access Patterns     | 8 GB    |

### 4.2 Experimental Approach

我们在 4.3 评估了页面跟踪的开销以及优化页表扫描带来的性能提升，在 4.4 

### 4.3 page tracking

#### 4.3.1 argument and overhead

自适应间隔的页表扫描策略需要确定三个参数，调节系数 SCAN_K 和 SCAN_K_BASE, 分别控制扫描时间 T_scan 的二次项和一次项权重，决定扫描间隔对扫描成本的敏感度;SCAN_BASE_INTERVAL 是基础扫描间隔，用于保障在低负载或低页表扫描时系统仍可定期识别热页。

我们首先通过在 1GB 活跃负载下使用固定扫描间隔（1000ms、1500ms、2000ms、2500ms）进行评估。结果显示 2500ms 在性能开销和热页识别率之间取得了良好平衡，因而被选定为 SCAN_BASE_INTERVAL。

![](./result/track/base_interval/base_scan_interval.png)

在确定基础间隔之后，我们进一步探索了调节系数 SCAN_K 对动态负载适应性的影响。此时将 SCAN_K_BASE 设置为 0，仅评估 SCAN_K 二次项的影响. 实验结果如图，当 SCAN_K 设置为 7 时，在大部分负载下系统开销超出控制阈值（5%）；当 SCAN_K 为 8 时仅有极少数情况超出阈值，而 SCAN_K = 9 时在所有测试中均满足约束。考虑到我们希望在尽可能较高的扫描频率下保证系统开销控制在 5% 以下，最终选择 SCAN_K = 8，随后，我们将 SCAN_K_BASE 设置为 50，进一步提升扫描间隔在不同 T_scan 情况下的调整弹性。实验表明，在该配置下所有测试负载场景中系统开销均保持在安全范围之内

![](./result/track/adaptive_scan/adaptive_overhead_bar.png)

在该配置下，我们对多个 benchmark 工作负载进行评估，结果显示系统性能开销均低于 5%，验证了参数配置的有效性与通用性

![](./result/track/overhead/benchmark_overhead.png)

我们也设计实验评估了线性调整策略，即 T_interval = SCAN_K * T_scan + SCAN_BASE_INTERVAL，依然选定 2500 作为 SCAN_BASE_INTERVAL。尽管随着系数SCAN_K 的增加，程序因页表扫描带来的性能开销（overhead）有所下降，但在高活跃工作负载条件下，该开销依然呈现持续上升趋势，未能有效遏制页面扫描对进程性能的负面影响。在大规模工作负载场景下，页表扫描时长增长幅度显著，而线性间隔调整的响应速度与幅度均不足，导致扫描频率仍然偏高，从而加剧对前台应用的干扰

![](./result/track/linear_not_good/linear_scan_k.png)

#### 4.3.2 page table scan optimization

我们在多个 benchmark 上对比评估了默认页表扫描模式（walk_page）与优化页表扫描模式（walk_page_opt）的性能差异。

下图展示了 redis 场景下的对比结果：两种扫描方式最终识别出的活跃页表项数量一致，但 walk_page_opt 通过跳过未访问路径，大幅减少了约 10% 的扫描范围，有效降低了扫描成本。

![](./result/track/pt_scan/pt_scan_redis_bar.png)

进一步，我们在六个典型 benchmark 上进行了系统评估。实验结果如图所示，在所有 benchmark 上均减小了 PTE 扫描范围和扫描时间, redis、xsbench、pr 和 gups 均为访问频繁、局部性较强的内存密集型应用，在这类场景中优化效果相对有限。而 graph500 和 liblinear 的性能提升最为显著：因为 Graph500 主要用于评测大规模图处理的性能,它的核心算法是 BFS(Breadth-First Search,广度优先搜索),Graph500 需要遍历大型图的数据结构(通常是邻接表或压缩格式存储),访问模式呈非连续、非局部特性, 这导致其访存模式是高度稀疏的。liblinear 则涉及大量稀疏数据的加载和处理，其训练过程中的内存访问分布同样稀疏，优化扫描策略可以显著减少冗余页表项的遍历开销。

walk_page_opt 能够通过剪枝未访问的上层页表路径，在不牺牲热页识别准确性的前提下，显著缩小扫描范围、降低扫描开销。优化效果随着应用访问稀疏性增强而越发明显，验证了该方法对图计算、稀疏矩阵计算等典型场景的适用性和通用价值。

![](./result/track/pt_scan/pt_scan_opt_all.png)

### 4.4 page migration

我们在系统平台上评估了多种分层内存方案在页面迁移速度方面的性能表现。实验设置如下：将 CPU 绑定至本地 NUMA 节点（node0），并在两个远程 CXL 节点（node2 和 node3）上各分配 2GB 内存。随后持续对分配区域执行写操作以触发页面访问，每隔 1 秒统计一次页面在各 NUMA 节点上的分布情况，Y 轴表示当前在 node0 上成功迁移的页面总大小。

随着访存行为的发生，CXL 内存上的访问触发 numa_hint_fault，导致页面开始被迁移至本地节点。所有分层内存管理策略在同一时间启动页面提升操作，即 node2→node0 与 node3→node0 的页面迁移流程。其中，vtism 显著优于其他方案，表现出最快的页面迁移速度。进一步分析发现，autonuma 和 tpp 均采用了 Linux 默认的同步迁移机制，迁移速度相对稳定但较慢；nomad 与 vtism 都基于工作队列机制实现异步页面迁移

![](./result/migration/promote/promote.png)

下图展示了在两个远程节点同时迁移 2/3 内存至本地节点的实验结果。可以看到，vtism 的迁移速度高于 nomad，因为两者存在关键实现差异：nomad 仅使用一个全局迁移工作队列处理所有节点的迁移任务，缺乏并行性支持，在多个 NUMA 节点同时发起大量迁移请求时容易成为性能瓶颈。
相比之下，vtism 为每个 NUMA 节点分别设置了独立的 promote（提升）队列，并将工作队列线程绑定到对应节点的本地 CPU 上，同时使用 WQ_HIGHPRI 设置高优先级以确保关键热页能被优先迁移，从而显著提升了迁移效率。

![](./result/migration/double_promote/promote_compare.png)

<!-- ```bash
numactl --cpubind=0 --membind=2 ./mm-mem/bin/cpu_loaded_latency
```

```bash
cat /proc/vmstat | grep numa
taskset -cp $(pidof cpu_loaded_latency)
numastat -p $(pidof cpu_loaded_latency)
``` -->

### 4.5 page classification

[TODO] 修改后的 MGLRU 的页面类型识别是准确的

MGLRU - MGLRU-Enhanced

![](./result/benchmark/benchmark_compare.png)

## 5. Related Work

Telescope 采用了相似的优化思路，但其修改 [https://lkml.org/lkml/2024/3/18/547] 仅作用在 DAMON 中, 也无法兼容各种版本的虚拟机内核，walk_page_vma_opt 具备更广泛的适用性,保持了和原API相同的接口, 能够在各个版本内核上无缝适配所有依赖 A/D 位扫描的方案;


## 6. conclusion and future work
